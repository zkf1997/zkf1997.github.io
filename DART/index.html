<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="description" content="DART">
       <meta name="keywords" content="motion generation">
      <meta name="author" content="Kaifeng Zhao">
      <title>DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</title>

      <!--<meta property="og:title" content="EgoBody: ..." />-->
      <!--<meta property="og:description" content="EgoBody is a ... ">-->
      <!--<meta property="og:image" content="images/teaser.jpg" />-->

      <!--<meta name="twitter:card" content="summary_large_image" />-->
      <!--<meta name="twitter:title" content="EgoBody: ..." />-->
      <!--<meta name="twitter:description" content="EgoBody is a ..." />-->
      <!--<meta name="twitter:image" content="https://neuralbodies.github.io/LEAP/images/teaser1200x630.jpg" />-->
      <!--<meta name="twitter:image:alt" content="EgoBody" />-->

      <!-- Bootstrap core CSS -->
      <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
      <!-- Custom styles for this template -->
      <link href="css/scrolling-nav.css" rel="stylesheet">
      <!-- nice figures  -->
      <link rel="stylesheet" href="css/font-awesome.css">
      <!-- <link rel="icon" type="image/png" href="images/icon.png"> -->
      
      <style>
         .video-container-2x2 {
           display: grid;
           grid-template-columns: repeat(2, 1fr);
           grid-gap: 10px; /* Adjust the gap between videos */
         }
         
         .video-container-2x2 video {
           width: 100%; /* Adjust the width as needed */
         }

         .video-container-2 {
            display: flex;
            justify-content: space-between;
         }
         
         .video-container-2 video {
            width: 48%; /* Adjust the width as needed */
         }

         .caption-container {
            display: flex;
            justify-content: space-between;
            margin-top: 10px; /* Adjust the margin as needed */
         }
         
         .caption {
            width: 48%; /* Adjust the width as needed */
            text-align: center;
         }

         .video-container-3 {
            display: flex;
            justify-content: space-between;
         }
         
         .video-container-3 video {
            width: 32%; /* Adjust the width as needed */
         }

         .caption-container-3 {
            display: flex;
            justify-content: space-between;
            margin-top: 10px; /* Adjust the margin as needed */
         }

         .caption-container-3 caption {
            width: 32%; /* Adjust the width as needed */
            text-align: center;
         }

         .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
         }
  
       </style>
   </head>
   <body id="page-top">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
         <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">DART</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
               <ul class="navbar-nav ml-auto">
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#about">Overview</a>
                  </li>
                  <!-- <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#demo">Demo</a>
                  </li> -->
                  <!-- <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#video">Video</a>
                  </li> -->
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#method">Method</a>
                  </li>
                  
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#results">Results</a>
                  </li>      
                  
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#citation">Citation</a>
                  </li>

                  <!-- <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#related_projects">Related Projects</a>
                  </li> -->
                  <!-- <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#team">Team</a>
                  </li> -->
               </ul>
            </div>
         </div>
      </nav>


      <header class="bg-light text-black">
          <div class="container text-center">
             <h1>DART</h1>
             <h2>A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</h2><br>
              <div id="content">
          <div id="content-inner">

            <div class="section head">

                <div class="authors">
                    <h5>
                        <a href="https://vlg.inf.ethz.ch/team/Kaifeng-Zhao.html">Kaifeng Zhao</a>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Gen-Li.html">Gen Li</a>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>
                        <h5>
                </div>

                <div class="affiliations">
                    <h5>
                        <a href="https://ethz.ch/en.html">ETH ZÃ¼rich</a>&nbsp; &nbsp;&nbsp;
                        <h5>
                </div>


                <div class="downloads">
                    <br><h3>
                    <a class="publink" href="https://arxiv.org/abs/2410.05260" target="_blank" style="text-decoration: none"> arXiv <i class="fa fa-print"></i></a> &nbsp;
                    <!-- &nbsp;&nbsp;
                    <a class="publink" href="https://github.com/zkf1997/DIMOS" target="_blank" style="text-decoration: none"> Code <i class="fa fa-github"></i></a> -->
                    <h3>
                </div>

            </div>


            <!-- <br> -->


        </div>
      </header>


      <section id="about" class="about-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">

                  <br><br>
                  <h2>Overview</h2>
                  <p class="lead text-justify">
                     In this work, we present <b>DART</b>, a <b>D</b>iffusion-based <b>A</b>utoregressive motion model for <b>R</b>eal-time <b>T</b>ext-driven motion control. 
                     DART achieves high-quality and efficient (<b> > 300 frames per second </b> on single RTX 4090 GPU) motion generation conditioned on online streams of text prompts. 
                     Furthermore, by integrating latent space optimization and reinforcement learning-based controls, DART enables various motion generation applications with <b>spatial constraints and goals</b>, including motion in-between, waypoint goal reaching, and human-scene interaction generation.
                  </p>
                 
                  <!-- <p><img class="img-fluid" alt="teaser" width="100%" src="images/teaser_canonical.png"></p> -->
               </div>
            </div>
         </div>
      </section>

      <section id="method" class="method-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <br>
                  <h2>Method</h2>
                  <!-- <p class="lead text-justify">
                     We formulate synthesizing human behaviors in 3D scenes as a Markov decision process with a latent action space, which is learned from motion capture datasets.
                     We train scene-aware and goal-driven agent policies to synthesize various human behaviors in indoor scenes including wandering in the room, sitting or lying on an object, and sequential combinations of these actions.
                  </p>
                  <br>
                  <br> -->
                  <h3>DART</h3>
                  <p><img class="img-fluid" alt="pipeline" width="100%" src="images/architechture.png"></p>
                  <p class="lead text-justify">
                     DART uses an architechture of autoregressive latent diffusion to learn a text-conditioned space of motion primitives. The encoder and decoder networks are frozen during the training of the denoiser network.
                  </p>
                  <br>
                  <br>
                  <p class="lead text-justify">
                     Illustration of online text-to-motion using DART:
                  </p>
                  <figure>
                     <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                         <source src="videos/rollout.mp4" type="video/mp4">
                     </video>
                 </figure>
                 <p class="lead text-justify">
                  An example rollout result:
               </p>
                  <figure>
                     <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                         <source src="videos/text_move_cam.mp4" type="video/mp4">
                     </video>
                 </figure>
                 <br>
                 <br>

                 <h3>Motion control in latent space</h3>
                 <p class="lead text-justify">
                  We formulate motion control as a minimization problem, aiming to identify motion sequences that are closest to the spatial objectives while adhering to regularization terms derived from scene and physical constraints.
                  We explore two methods leveraging the latent motion space of DART to solve this problem: gradient descent-based optimization and reinforcement learning within the learned latent space. 
               </p>
                  <center><p><img class="img-fluid" alt="pipeline" width="75%" src="images/latent_control.png"></p></center>
                  <p class="lead text-center">
                     Formulation of motion generation with spatial goals and constraints.
                  </p>
                  <br>
                  <br>
                  <p><img class="img-fluid" alt="pipeline" width="100%" src="images/rl.png"></p>
                  <p class="lead text-center">
                     Illustration of the reinforcement-learning policy-based control.
                  </p>
                  
               </div>
            </div>
         </div>
      </section>

      <!-- <section id="video" class="video-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Video</h2>
                  <div class="embed-responsive embed-responsive-16by9">
                    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/O3VpvETNjcw" title="DIMOS" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                  </div>
                  
               </div>
            </div>
         </div>
      </section> -->

      <section id="results" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Results</h2>
                  <h3>Text-conditioned temporal motion composition</h3>
                  <figure>
                     <video class="centered" width="100%" controls="" muted="" loop=true autoplay=true>
                         <source src="videos/text_compare.mp4" type="video/mp4">
                     </video>
                 </figure>
                 <h4>Command-line text-driven interactive demo</h4>
                 <figure>
                  <video class="centered" width="100%" controls="" muted="" loop=true autoplay=true>
                      <source src="videos/0927.mp4" type="video/mp4">
                  </video>
              </figure>

                 <h3>Optimization-based control</h3>
                 <!-- <br> -->
                 <h4>Text-conditioned motion in-between</h4>
                 <p class="lead text-justify">
                  Integrating DART with latent space optimization-based control enables the generation of high-quality intermediate motions that smoothly transition between the keyframes conditioned on the text semantics. 
                  Our method outperforms the baseline <a href="https://korrawe.github.io/dno-project/">DNO</a>, particularly in terms of <b>semantic alignment with the text prompts</b>.
                </p>
                 <!-- <div class="video-container-3">
                  <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                     <source src="videos/inbetween/walk our.mp4" type="video/mp4">
                  </video>
                
                  <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                     <source src="videos/inbetween/jump_ours.mp4" type="video/mp4">
                  </video>
                          
                  <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                     <source src="videos/inbetween/climb down our.mp4" type="video/mp4">
                  </video>
                  
            </div>
            <div class="caption-container-3">
               <div class="caption">"walk"</div>
               <div class="caption">"jump forward"</div>
               <div class="caption">"climb down stairs"</div>
             </div> -->
             <!-- <br> -->
             
             <!-- <p class="lead text-justify">
               Our method outperforms the baseline <a href="https://korrawe.github.io/dno-project/">DNO</a>, particularly in terms of <b>semantic alignment with the text prompts</b>.
             </p> -->
            <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/pace_our.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/pace_dno.mp4" type="video/mp4">
               </video>
            </div>
            <div class="caption-container">
               <div class="caption">Ours</div>
               <div class="caption"><a href="https://korrawe.github.io/dno-project/">DNO</a></div>
            </div>
            <p class="lead text-center">
               "A person paces in circles", duration 4 seconds
             </p>

             <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/dance_our.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/dance_dno.mp4" type="video/mp4">
               </video>
            </div>
            <div class="caption-container">
               <div class="caption">Ours</div>
               <div class="caption"><a href="https://korrawe.github.io/dno-project/">DNO</a></div>
            </div>
            <p class="lead text-center">
               "A person dances", duration 4 seconds (initial frame not visualized to reduce occlusion)
             </p>

             <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/crawl_our.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/crawl_dno.mp4" type="video/mp4">
               </video>
            </div>
            <div class="caption-container">
               <div class="caption">Ours</div>
               <div class="caption"><a href="https://korrawe.github.io/dno-project/">DNO</a></div>
            </div>
            <p class="lead text-center">
               "A person crawls", duration 4 seconds (initial frame not visualized to reduce occlusion)
             </p>

             <div class="video-container-2">
             <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                <source src="videos/inbetween_20fps/climbdown_our_light.mp4" type="video/mp4">
             </video>
             
             <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                <source src="videos/inbetween_20fps/climbdown_omnicontrol_light.mp4" type="video/mp4">
             </video>
          </div>
          <div class="caption-container">
             <div class="caption">Ours</div>
             <div class="caption"><a href="https://neu-vi.github.io/omnicontrol/">OmniControl</a></div>
          </div>
          <p class="lead text-center">
             "A person climbs down stairs", duration 2.25 seconds 
           </p>

             <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/crawl_side_our.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/inbetween_20fps/crawl_side_omnicontrol.mp4" type="video/mp4">
               </video>
            </div>
            <div class="caption-container">
               <div class="caption">Ours</div>
               <div class="caption"><a href="https://neu-vi.github.io/omnicontrol/">OmniControl</a></div>
            </div>
            <p class="lead text-center">
               "A person crawls", duration 4 seconds 
             </p>

            
            <br>
            <br>
            <h4>Human-scene interaction</h4>
            <p class="lead text-justify">
            We present preliminary results of synthesizing human-scene interactions by inputting text prompts, scene SDF (signed distance field), and a goal pelvis joint location. The red sphere in the visualizations represents the given goal pelvis joint location.
             </p>
            <div class="video-container-3">
                 <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/interaction/sit.mp4" type="video/mp4">
               </video>
             
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/interaction/climb up.mp4" type="video/mp4">
               </video>
                       
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/interaction/climb down 1.mp4" type="video/mp4">
               </video> 
                  
            </div>
            <div class="caption-container-3">
               <div class="caption">walk-turn left-sit on the chair</div>
               <div class="caption">climb up stairs</div>
               <div class="caption">climb down stairs</div>
             </div>
             
            <br>
                 
            <h3>Reinforcement learning-based control</h3>
            <h4>Text-conditioned waypoint goal reaching</h4>
            <p class="lead text-justify">
               The waypoint reaching task requires the human agent to sequentially reach dynamically updated goals, each visualized as a yellow torus. A new goal waypoint is given to the human agent after the current waypoint has been successfully reached.
               <!-- <br> -->
               Our method, combining DART with RL-based control, can generate <b>240 frames per second </b>when tested on a single RTX 4090 GPU.
                <!-- and it outperforms the baseline <a href="https://yz-cnsdqz.github.io/eigenmotion/GAMMA/">GAMMA</a> in motion quality and goal reach success rate. -->
               Moreover, our method enables the selection of locomotion styles by conditioning the policy with various text prompts, a capability that extends beyond the scope of existing works.
            </p>
            <!-- <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/goal_reach/walk.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/goal_reach/gamma.mp4" type="video/mp4">
               </video>
            </div>
            <div class="caption-container">
               <div class="caption">Ours: "walk"</div>
               <div class="caption"><a href="https://yz-cnsdqz.github.io/eigenmotion/GAMMA/">GAMMA</a></div>
               </div> -->

            
            
            <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/goal_reach/run.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/goal_reach/run_10.mp4" type="video/mp4">
               </video>
            </div>
            <p class="lead text-center">
               Ours: "run"
               </p>

               <div class="video-container-2">
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/goal_reach/hop long.mp4" type="video/mp4">
               </video>
               
               <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                  <source src="videos/goal_reach/hop.mp4" type="video/mp4">
               </video>
            </div>
            <p class="lead text-center">
               Ours: "hop on left leg"
               </p>
               
               <!-- <div class="video-container-2">
                  <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                     <source src="videos/goal_reach/random_walk.mp4" type="video/mp4">
                  </video>
               
                  <video class="centered" width="100%" controls="" muted="" loop="" autoplay="">
                     <source src="videos/goal_reach/random_run.mp4" type="video/mp4">
                  </video>
               </div>
               <div class="caption-container">
                  <div class="caption">Ours: "walk"</div>
                  <div class="caption">Ours: "run"</div>
               </div> -->
               </div>
            </div>
         </div>
      </section>

      <section id="citation" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Citation</h2>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@inproceedings{Zhao:DART:2024,
   title = {A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control},
   author = {Zhao, Kaifeng and Li, Gen and Tang, Siyu},
   year = {2024}
}</pre>
         <!-- <h2>Related Projects</h2>
         <p class="lead text-justify">
         This project is developed on top of prior works, please also consider citing the following projects:
         </p>

<a href="https://yz-cnsdqz.github.io/eigenmotion/GAMMA/">GAMMA: The Wanderings of Odysseus in 3D Scenes</a>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@inproceedings{zhang2022wanderings,
   title={The Wanderings of Odysseus in 3D Scenes},
   author={Zhang, Yan and Tang, Siyu},
   booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
   pages={20481--20491},
   year={2022}
}</pre>

<a href="https://zkf1997.github.io/COINS/index.html">COINS: Compositional Human-Scene Interaction Synthesis with Semantic Control</a>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
   @inproceedings{Zhao:ECCV:2022,
   title = {Compositional Human-Scene Interaction Synthesis with Semantic Control},
   author = {Zhao, Kaifeng and Wang, Shaofei and Zhang, Yan and Beeler, Thabo and Tang, Siyu},
   booktitle = {European conference on computer vision (ECCV)},
   year = {2022}
}</pre> -->
               </div>
            </div>
         </div>
      </section>

      <!-- <section id="related_projects" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
	              
                    
               </div>
            </div>
         </div>
      </section> -->

      <section id="contact" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
	                <h2>Contact</h2>
		            <br>For questions, please contact Kaifeng Zhao:<br><a href="mailto:kaifeng.zhao@inf.ethz.ch">kaifeng.zhao@inf.ethz.ch</a>
               </div>
            </div>
         </div>
      </section>

      <!-- Footer -->
      <footer class="py-5 bg-dark">
         <div class="container">
            <p class="m-0 text-center text-white">Copyright &copy; VLG 2024</p>
             <p style="text-align:right;font-size:small;" class="text-white">
            template from <a href="https://neuralbodies.github.io/LEAP/index.html">LEAP</a>
         </div>
         <!-- /.container -->
      </footer>
      <!-- Bootstrap core JavaScript -->
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
      <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
      <!-- Custom JavaScript for this theme -->
      <script src="js/scrolling-nav.js"></script>
   </body>
</html>
